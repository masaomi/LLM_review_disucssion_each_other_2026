{
  "task_id": "code_generation_001",
  "models": {
    "opus_4_5": {
      "task_id": "code_generation_001",
      "model_key": "opus_4_5",
      "model_id": "anthropic/claude-opus-4.5",
      "response": "# LRU Cache Implementation in Python\n\n## Overview\n\nI'll implement an LRU Cache using a **doubly linked list** combined with a **hash map (dictionary)**. This combination allows O(1) access, insertion, and deletion operations.\n\n## Implementation\n\n```python\nfrom typing import TypeVar, Generic, Optional, Dict\n\nK = TypeVar('K')\nV = TypeVar('V')\n\n\nclass Node(Generic[K, V]):\n    \"\"\"\n    A node in the doubly linked list.\n    \n    Stores key-value pair and maintains pointers to previous and next nodes.\n    The key is stored to enable O(1) removal from the hash map during eviction.\n    \"\"\"\n    \n    def __init__(self, key: K, value: V) -> None:\n        self.key: K = key\n        self.value: V = value\n        self.prev: Optional['Node[K, V]'] = None\n        self.next: Optional['Node[K, V]'] = None\n\n\nclass DoublyLinkedList(Generic[K, V]):\n    \"\"\"\n    A doubly linked list with sentinel head and tail nodes.\n    \n    The list maintains order from most recently used (after head)\n    to least recently used (before tail).\n    \n    Sentinel nodes simplify edge case handling for insertions and deletions.\n    \"\"\"\n    \n    def __init__(self) -> None:\n        \"\"\"Initialize the list with sentinel head and tail nodes.\"\"\"\n        # Using None as placeholder for sentinel nodes' key/value\n        self.head: Node[K, V] = Node(None, None)  # type: ignore\n        self.tail: Node[K, V] = Node(None, None)  # type: ignore\n        self.head.next = self.tail\n        self.tail.prev = self.head\n    \n    def add_to_front(self, node: Node[K, V]) -> None:\n        \"\"\"\n        Add a node right after the head (most recently used position).\n        \n        Args:\n            node: The node to add to the front of the list.\n        \n        Time Complexity: O(1)\n        \"\"\"\n        node.prev = self.head\n        node.next = self.head.next\n        self.head.next.prev = node\n        self.head.next = node\n    \n    def remove(self, node: Node[K, V]) -> None:\n        \"\"\"\n        Remove a node from anywhere in the list.\n        \n        Args:\n            node: The node to remove from the list.\n        \n        Time Complexity: O(1)\n        \"\"\"\n        node.prev.next = node.next\n        node.next.prev = node.prev\n        node.prev = None\n        node.next = None\n    \n    def move_to_front(self, node: Node[K, V]) -> None:\n        \"\"\"\n        Move an existing node to the front of the list.\n        \n        Args:\n            node: The node to move to the front.\n        \n        Time Complexity: O(1)\n        \"\"\"\n        self.remove(node)\n        self.add_to_front(node)\n    \n    def remove_last(self) -> Optional[Node[K, V]]:\n        \"\"\"\n        Remove and return the last node (least recently used).\n        \n        Returns:\n            The removed node, or None if the list is empty.\n        \n        Time Complexity: O(1)\n        \"\"\"\n        if self.tail.prev == self.head:\n            return None  # List is empty\n        \n        last_node = self.tail.prev\n        self.remove(last_node)\n        return last_node\n\n\nclass LRUCache(Generic[K, V]):\n    \"\"\"\n    A Least Recently Used (LRU) Cache implementation.\n    \n    This cache stores key-value pairs up to a specified capacity.\n    When the cache is full and a new item is added, the least\n    recently used item is evicted.\n    \n    Both get and put operations run in O(1) time complexity.\n    \n    Implementation uses:\n    - A hash map for O(1) key lookup\n    - A doubly linked list to maintain access order\n    \n    Attributes:\n        capacity: Maximum number of items the cache can hold.\n    \n    Example:\n        >>> cache = LRUCache(2)\n        >>> cache.put(\"a\", 1)\n        >>> cache.put(\"b\", 2)\n        >>> cache.get(\"a\")\n        1\n        >>> cache.put(\"c\", 3)  # Evicts \"b\" as it's least recently used\n        >>> cache.get(\"b\")  # Returns None, \"b\" was evicted\n    \"\"\"\n    \n    def __init__(self, capacity: int) -> None:\n        \"\"\"\n        Initialize the LRU Cache with a given capacity.\n        \n        Args:\n            capacity: The maximum number of items the cache can store.\n                     Must be a positive integer.\n        \n        Raises:\n            ValueError: If capacity is not a positive integer.\n        \"\"\"\n        if capacity <= 0:\n            raise ValueError(\"Capacity must be a positive integer\")\n        \n        self.capacity: int = capacity\n        self._cache: Dict[K, Node[K, V]] = {}\n        self._list: DoublyLinkedList[K, V] = DoublyLinkedList()\n    \n    def get(self, key: K) -> Optional[V]:\n        \"\"\"\n        Retrieve a value from the cache by key.\n        \n        If the key exists, the item is marked as recently used.\n        \n        Args:\n            key: The key to look up in the cache.\n        \n        Returns:\n            The value associated with the key, or None if not found.\n        \n        Time Complexity: O(1)\n        Space Complexity: O(1)\n        \"\"\"\n        if key not in self._cache:\n            return None\n        \n        node = self._cache[key]\n        # Mark as recently used by moving to front\n        self._list.move_to_front(node)\n        return node.value\n    \n    def put(self, key: K, value: V) -> None:\n        \"\"\"\n        Insert or update a key-value pair in the cache.\n        \n        If the key already exists, its value is updated and it's marked\n        as recently used. If the cache is at capacity and a new key is\n        added, the least recently used item is evicted.\n        \n        Args:\n            key: The key to insert or update.\n            value: The value to associate with the key.\n        \n        Time Complexity: O(1)\n        Space Complexity: O(1)\n        \"\"\"\n        if key in self._cache:\n            # Update existing key\n            node = self._cache[key]\n            node.value = value\n            self._list.move_to_front(node)\n        else:\n            # Add new key\n            if len(self._cache) >= self.capacity:\n                # Evict least recently used item\n                evicted_node = self._list.remove_last()\n                if evicted_node:\n                    del self._cache[evicted_node.key]\n            \n            # Create and add new node\n            new_node = Node(key, value)\n            self._cache[key] = new_node\n            self._list.add_to_front(new_node)\n    \n    def __len__(self) -> int:\n        \"\"\"Return the current number of items in the cache.\"\"\"\n        return len(self._cache)\n    \n    def __contains__(self, key: K) -> bool:\n        \"\"\"Check if a key exists in the cache (without updating access order).\"\"\"\n        return key in self._cache\n    \n    def __repr__(self) -> str:\n        \"\"\"Return a string representation of the cache.\"\"\"\n        items = []\n        current = self._list.head.next\n        while current != self._list.tail:\n            items.append(f\"{current.key}: {current.value}\")\n            current = current.next\n        return f\"LRUCache(capacity={self.capacity}, items=[{', '.join(items)}])\"\n```\n\n## Unit Tests\n\n```python\nimport unittest\n\n\nclass TestLRUCache(unittest.TestCase):\n    \"\"\"Comprehensive unit tests for LRU Cache implementation.\"\"\"\n    \n    def test_basic_get_and_put(self):\n        \"\"\"Test basic get and put operations.\"\"\"\n        cache: LRUCache[str, int] = LRUCache(3)\n        \n        # Test put and get\n        cache.put(\"a\", 1)\n        cache.put(\"b\", 2)\n        cache.put(\"c\", 3)\n        \n        self.assertEqual(cache.get(\"a\"), 1)\n        self.assertEqual(cache.get(\"b\"), 2)\n        self.assertEqual(cache.get(\"c\"), 3)\n        self.assertIsNone(cache.get(\"d\"))  # Non-existent key\n        \n        # Test update existing key\n        cache.put(\"a\", 100)\n        self.assertEqual(cache.get(\"a\"), 100)\n        \n        # Verify size\n        self.assertEqual(len(cache), 3)\n    \n    def test_eviction_policy(self):\n        \"\"\"Test that least recently used items are evicted correctly.\"\"\"\n        cache: LRUCache[str, int] = LRUCache(3)\n        \n        cache.put(\"a\", 1)\n        cache.put(\"b\", 2)\n        cache.put(\"c\", 3)\n        \n        # Access \"a\" to make it recently used\n        cache.get(\"a\")\n        \n        # Add new item, should evict \"b\" (least recently used)\n        cache.put(\"d\", 4)\n        \n        self.assertIsNone(cache.get(\"b\"))  # \"b\" should be evicted\n        self.assertEqual(cache.get(\"a\"), 1)  # \"a\" should still exist\n        self.assertEqual(cache.get(\"c\"), 3)  # \"c\" should still exist\n        self.assertEqual(cache.get(\"d\"), 4)  # \"d\" should exist\n    \n    def test_update_moves_to_front(self):\n        \"\"\"Test that updating a key moves it to most recently used.\"\"\"\n        cache: LRUCache[str, int] = LRUCache(2)\n        \n        cache.put(\"a\", 1)\n        cache.put(\"b\", 2)\n        \n        # Update \"a\", making it most recently used\n        cache.put(\"a\", 10)\n        \n        # Add new item, should evict \"b\" (now least recently used)\n        cache.put(\"c\", 3)\n        \n        self.assertIsNone(cache.get(\"b\"))  # \"b\" should be evicted\n        self.assertEqual(cache.get(\"a\"), 10)  # \"a\" should exist with updated value\n        self.assertEqual(cache.get(\"c\"), 3)  # \"c\" should exist\n    \n    def test_capacity_one(self):\n        \"\"\"Test edge case with capacity of 1.\"\"\"\n        cache: LRUCache[str, int] = LRUCache(1)\n        \n        cache.put(\"a\", 1)\n        self.assertEqual(cache.get(\"a\"), 1)\n        \n        cache.put(\"b\", 2)\n        self.assertIsNone(cache.get(\"a\"))  # \"a\" should be evicted\n        self.assertEqual(cache.get(\"b\"), 2)\n    \n    def test_access_order_matters(self):\n        \"\"\"Test that access order determines eviction priority.\"\"\"\n        cache: LRUCache[int, str] = LRUCache(3)\n        \n        cache.put(1, \"one\")\n        cache.put(2, \"two\")\n        cache.put(3, \"three\")\n        \n        # Access in specific order: 2, 1, 3\n        cache.get(2)\n        cache.get(1)\n        cache.get(3)\n        \n        # Now order is (most recent to least): 3, 1, 2\n        # Adding new item should evict 2\n        cache.put(4, \"four\")\n        \n        self.assertIsNone(cache.get(2))  # 2 should be evicted\n        self.assertEqual(cache.get(1), \"one\")\n        self.assertEqual(cache.get(3), \"three\")\n        self.assertEqual(cache.get(4), \"four\")\n    \n    def test_invalid_capacity(self):\n        \"\"\"Test that invalid capacity raises ValueError.\"\"\"\n        with self.assertRaises(ValueError):\n            LRUCache(0)\n        \n        with self.assertRaises(ValueError):\n            LRUCache(-1)\n    \n    def test_contains_and_len(self):\n        \"\"\"Test __contains__ and __len__ methods.\"\"\"\n        cache: LRUCache[str, int] = LRUCache(2)\n        \n        self.assertEqual(len(cache), 0)\n        self.assertNotIn(\"a\", cache)\n        \n        cache.put(\"a\", 1)\n        self.assertEqual(len(cache), 1)\n        self.assertIn(\"a\", cache)\n        \n        cache.put(\"b\", 2)\n        self.assertEqual(len(cache), 2)\n        \n        cache.put(\"c\", 3)  # Evicts \"a\"\n        self.assertEqual(len(cache), 2)\n        self.assertNotIn(\"a\", cache)\n        self.assertIn(\"b\", cache)\n        self.assertIn(\"c\", cache)\n    \n    def test_different_types(self):\n        \"\"\"Test cache with different key and value types.\"\"\"\n        # Integer keys, string values\n        cache1: LRUCache[int, str] = LRUCache(2)\n        cache1.put(1, \"hello\")\n        cache1.put(2, \"world\")\n        self.assertEqual(cache1.get(1), \"hello\")\n        \n        # Tuple keys, list values\n        cache2: LRUCache[tuple, list] = LRUCache(2)\n        cache2.put((1, 2), [1, 2, 3])\n        cache2.put((3, 4), [4, 5, 6])\n        self.assertEqual(cache2.get((1, 2)), [1, 2, 3])\n\n\nclass TestDoublyLinkedList(unittest.TestCase):\n    \"\"\"Unit tests for the DoublyLinkedList helper class.\"\"\"\n    \n    def test_add_and_remove(self):\n        \"\"\"Test adding and removing nodes.\"\"\"\n        dll: DoublyLinkedList[str, int] = DoublyLinkedList()\n        \n        node1 = Node(\"a\", 1)\n        node2 = Node(\"b\", 2)\n        \n        dll.add_to_front(node1)\n        dll.add_to_front(node2)\n        \n        # node2 should be at front\n        self.assertEqual(dll.head.next, node2)\n        self.assertEqual(dll.tail.prev, node1)\n        \n        dll.remove(node2)\n        self.assertEqual(dll.head.next, node1)\n    \n    def test_remove_last(self):\n        \"\"\"Test removing the last (LRU) node.\"\"\"\n        dll: DoublyLinkedList[str, int] = DoublyLinkedList()\n        \n        node1 = Node(\"a\", 1)\n        node2 = Node(\"b\", 2)\n        \n        dll.add_to_front(node1)\n        dll.add_to_front(node2)\n        \n        removed = dll.remove_last()\n        self.assertEqual(removed, node1)\n        self.assertEqual(dll.tail.prev, node2)\n    \n    def test_remove_last_empty(self):\n        \"\"\"Test removing from empty list returns None.\"\"\"\n        dll: DoublyLinkedList[str, int] = DoublyLinkedList()\n        self.assertIsNone(dll.remove_last())\n\n\nif __name__ == \"__main__\":\n    unittest.main(verbosity=2)\n```\n\n## Complexity Analysis\n\n### Time Complexity\n\n| Operation | Time Complexity | Explanation |\n|-----------|-----------------|-------------|\n| `get(key)` | **O(1)** | Hash map lookup is O(1). Moving node to front in doubly linked list is O(1) since we have direct pointer access. |\n| `put(key, value)` | **O(1)** | Hash map insertion/lookup is O(1). Adding/removing nodes in doubly linked list is O(1) due to direct pointer manipulation. |\n\n### Space Complexity\n\n| Aspect | Space Complexity | Explanation |\n|--------|------------------|-------------|\n| Overall | **O(n)** | Where n is the cache capacity. We store at most n nodes in the linked list and n entries in the hash map. |\n| Per Entry | **O(1)** | Each entry requires constant space for the node (key, value, two pointers) and hash map entry",
      "latency_ms": 44846.99883400026,
      "usage": {
        "prompt_tokens": 179,
        "completion_tokens": 4096,
        "total_tokens": 4275
      },
      "timestamp": 1770394580.334987,
      "error": null
    },
    "opus_4_6": {
      "task_id": "code_generation_001",
      "model_key": "opus_4_6",
      "model_id": "anthropic/claude-opus-4.6",
      "response": "\n\n# LRU Cache Implementation\n\n## Implementation\n\n```python\n\"\"\"\nLRU (Least Recently Used) Cache implementation using a doubly linked list\nand a hash map for O(1) get and put operations.\n\"\"\"\n\nfrom typing import Optional, Any, Dict\n\n\nclass _Node:\n    \"\"\"A node in the doubly linked list used internally by the LRU Cache.\n    \n    Attributes:\n        key: The key associated with this node (needed for eviction lookup).\n        value: The value stored in this node.\n        prev: Pointer to the previous node in the linked list.\n        next: Pointer to the next node in the linked list.\n    \"\"\"\n    \n    __slots__ = ('key', 'value', 'prev', 'next')\n    \n    def __init__(self, key: Any = None, value: Any = None) -> None:\n        self.key = key\n        self.value = value\n        self.prev: Optional['_Node'] = None\n        self.next: Optional['_Node'] = None\n\n\nclass LRUCache:\n    \"\"\"A Least Recently Used (LRU) Cache with O(1) get and put operations.\n    \n    The cache combines a doubly linked list with a hash map:\n    - The doubly linked list maintains access order (most recent at head,\n      least recent at tail).\n    - The hash map provides O(1) key-to-node lookup.\n    \n    When the cache reaches capacity, the least recently used item (at the tail)\n    is evicted to make room for new entries.\n    \n    Attributes:\n        capacity: Maximum number of items the cache can hold.\n    \n    Example:\n        >>> cache = LRUCache(2)\n        >>> cache.put(\"a\", 1)\n        >>> cache.put(\"b\", 2)\n        >>> cache.get(\"a\")\n        1\n        >>> cache.put(\"c\", 3)  # Evicts \"b\" (least recently used)\n        >>> cache.get(\"b\")     # Returns None (evicted)\n    \"\"\"\n    \n    def __init__(self, capacity: int) -> None:\n        \"\"\"Initialize the LRU Cache.\n        \n        Args:\n            capacity: The maximum number of key-value pairs the cache can store.\n                      Must be a positive integer.\n        \n        Raises:\n            ValueError: If capacity is not a positive integer.\n        \"\"\"\n        if capacity <= 0:\n            raise ValueError(f\"Capacity must be a positive integer, got {capacity}\")\n        \n        self._capacity: int = capacity\n        self._size: int = 0\n        self._map: Dict[Any, _Node] = {}\n        \n        # Sentinel nodes simplify edge-case handling for the doubly linked list.\n        # The list structure is: head <-> [node1] <-> [node2] <-> ... <-> tail\n        # Most recently used nodes are near head; least recently used near tail.\n        self._head: _Node = _Node()  # dummy head\n        self._tail: _Node = _Node()  # dummy tail\n        self._head.next = self._tail\n        self._tail.prev = self._head\n    \n    @property\n    def capacity(self) -> int:\n        \"\"\"The maximum number of items the cache can hold.\"\"\"\n        return self._capacity\n    \n    @property\n    def size(self) -> int:\n        \"\"\"The current number of items in the cache.\"\"\"\n        return self._size\n    \n    def _remove_node(self, node: _Node) -> None:\n        \"\"\"Remove a node from its current position in the doubly linked list.\n        \n        This is an internal operation that only adjusts pointers; it does NOT\n        remove the node from the hash map or decrement the size counter.\n        \n        Args:\n            node: The node to remove from the list. Must not be a sentinel.\n        \"\"\"\n        prev_node = node.prev\n        next_node = node.next\n        prev_node.next = next_node\n        next_node.prev = prev_node\n    \n    def _add_to_front(self, node: _Node) -> None:\n        \"\"\"Insert a node right after the dummy head (marking it most recently used).\n        \n        This is an internal operation that only adjusts pointers; it does NOT\n        add the node to the hash map or increment the size counter.\n        \n        Args:\n            node: The node to insert at the front of the list.\n        \"\"\"\n        node.prev = self._head\n        node.next = self._head.next\n        self._head.next.prev = node\n        self._head.next = node\n    \n    def _move_to_front(self, node: _Node) -> None:\n        \"\"\"Move an existing node to the front (most recently used position).\n        \n        Args:\n            node: The node to move. Must already be in the list.\n        \"\"\"\n        self._remove_node(node)\n        self._add_to_front(node)\n    \n    def _evict_lru(self) -> None:\n        \"\"\"Remove the least recently used node (the one right before the dummy tail).\n        \n        This removes the node from both the linked list and the hash map,\n        and decrements the size counter.\n        \"\"\"\n        lru_node = self._tail.prev\n        if lru_node is self._head:\n            return  # List is empty; nothing to evict\n        \n        self._remove_node(lru_node)\n        del self._map[lru_node.key]\n        self._size -= 1\n    \n    def get(self, key: Any) -> Optional[Any]:\n        \"\"\"Retrieve the value associated with the given key.\n        \n        If the key exists, it is marked as most recently used.\n        \n        Args:\n            key: The key to look up in the cache.\n        \n        Returns:\n            The value associated with the key, or None if the key is not found.\n        \n        Time Complexity: O(1)\n        \"\"\"\n        node = self._map.get(key)\n        if node is None:\n            return None\n        \n        # Mark as most recently used\n        self._move_to_front(node)\n        return node.value\n    \n    def put(self, key: Any, value: Any) -> None:\n        \"\"\"Insert or update a key-value pair in the cache.\n        \n        If the key already exists, its value is updated and it is marked as\n        most recently used. If the key is new and the cache is at capacity,\n        the least recently used item is evicted before inserting the new item.\n        \n        Args:\n            key: The key to insert or update.\n            value: The value to associate with the key.\n        \n        Time Complexity: O(1)\n        \"\"\"\n        node = self._map.get(key)\n        \n        if node is not None:\n            # Key exists: update value and move to front\n            node.value = value\n            self._move_to_front(node)\n        else:\n            # Key is new: evict if at capacity, then insert\n            if self._size >= self._capacity:\n                self._evict_lru()\n            \n            new_node = _Node(key, value)\n            self._map[key] = new_node\n            self._add_to_front(new_node)\n            self._size += 1\n    \n    def __contains__(self, key: Any) -> bool:\n        \"\"\"Check if a key exists in the cache (does NOT affect access order).\n        \n        Args:\n            key: The key to check.\n        \n        Returns:\n            True if the key is in the cache, False otherwise.\n        \"\"\"\n        return key in self._map\n    \n    def __repr__(self) -> str:\n        \"\"\"Return a string representation showing items from most to least recently used.\"\"\"\n        items = []\n        current = self._head.next\n        while current is not self._tail:\n            items.append(f\"{current.key!r}: {current.value!r}\")\n            current = current.next\n        return f\"LRUCache(capacity={self._capacity}, items=[{', '.join(items)}])\"\n\n\n# ──────────────────────────────────────────────────────────────────────────────\n# Unit Tests\n# ──────────────────────────────────────────────────────────────────────────────\n\nimport unittest\n\n\nclass TestLRUCache(unittest.TestCase):\n    \"\"\"Comprehensive unit tests for the LRU Cache implementation.\"\"\"\n    \n    # ── Test 1: Basic get/put and eviction ────────────────────────────────\n    \n    def test_basic_get_put_and_eviction(self) -> None:\n        \"\"\"Test that basic get/put work and LRU eviction occurs correctly.\"\"\"\n        cache = LRUCache(2)\n        \n        # Put two items\n        cache.put(1, \"one\")\n        cache.put(2, \"two\")\n        self.assertEqual(cache.get(1), \"one\")   # key 1 is now most recent\n        \n        # Cache is full. Inserting key 3 should evict key 2 (the LRU item)\n        cache.put(3, \"three\")\n        self.assertIsNone(cache.get(2))          # key 2 was evicted\n        self.assertEqual(cache.get(3), \"three\")  # key 3 is present\n        self.assertEqual(cache.get(1), \"one\")    # key 1 is still present\n    \n    # ── Test 2: Updating existing key refreshes its position ──────────────\n    \n    def test_update_existing_key_refreshes_position(self) -> None:\n        \"\"\"Test that updating an existing key makes it most recently used.\"\"\"\n        cache = LRUCache(2)\n        \n        cache.put(\"a\", 1)\n        cache.put(\"b\", 2)\n        \n        # Update \"a\" — this should make \"a\" most recently used, \"b\" becomes LRU\n        cache.put(\"a\", 10)\n        self.assertEqual(cache.get(\"a\"), 10)  # value was updated\n        \n        # Insert \"c\" — should evict \"b\" (the LRU), not \"a\"\n        cache.put(\"c\", 3)\n        self.assertIsNone(cache.get(\"b\"))     # \"b\" was evicted\n        self.assertEqual(cache.get(\"a\"), 10)  # \"a\" still present\n        self.assertEqual(cache.get(\"c\"), 3)   # \"c\" present\n    \n    # ── Test 3: get() promotes item, preventing its eviction ──────────────\n    \n    def test_get_promotes_item_preventing_eviction(self) -> None:\n        \"\"\"Test that accessing a key via get() makes it most recently used.\"\"\"\n        cache = LRUCache(3)\n        \n        cache.put(1, \"a\")\n        cache.put(2, \"b\")\n        cache.put(3, \"c\")\n        \n        # Access key 1 — it becomes most recently used\n        # Order (MRU→LRU): 1, 3, 2\n        self.assertEqual(cache.get(1), \"a\")\n        \n        # Insert key 4 — should evict key 2 (the LRU)\n        cache.put(4, \"d\")\n        self.assertIsNone(cache.get(2))       # key 2 evicted\n        self.assertEqual(cache.get(1), \"a\")   # key 1 still present\n        self.assertEqual(cache.get(3), \"c\")   # key 3 still present\n        self.assertEqual(cache.get(4), \"d\")   # key 4 present\n    \n    # ── Test 4: Cache with capacity 1 ────────────────────────────────────\n    \n    def test_capacity_one(self) -> None:\n        \"\"\"Test edge case where cache capacity is 1.\"\"\"\n        cache = LRUCache(1)\n        \n        cache.put(\"x\", 100)\n        self.assertEqual(cache.get(\"x\"), 100)\n        \n        # Inserting another item should evict the only item\n        cache.put(\"y\", 200)\n        self.assertIsNone(cache.get(\"x\"))\n        self.assertEqual(cache.get(\"y\"), 200)\n    \n    # ── Test 5: get() on missing key returns None ─────────────────────────\n    \n    def test_get_missing_key_returns_none(self) -> None:\n        \"\"\"Test that get() returns None for keys not in the cache.\"\"\"\n        cache = LRUCache(5)\n        self.assertIsNone(cache.get(\"nonexistent\"))\n        \n        cache.put(\"exists\", 42)\n        self.assertIsNone(cache.get(\"does_not_exist\"))\n        self.assertEqual(cache.get(\"exists\"), 42)\n    \n    # ── Test 6: Invalid capacity raises ValueError ────────────────────────\n    \n    def test_invalid_capacity_raises_error(self) -> None:\n        \"\"\"Test that creating a cache with non-positive capacity raises ValueError.\"\"\"\n        with self.assertRaises(ValueError):\n            LRUCache(0)\n        with self.assertRaises(ValueError):\n            LRUCache(-5)\n    \n    # ── Test 7: Size tracking ─────────────────────────────────────────────\n    \n    def test_size_tracking(self) -> None:\n        \"\"\"Test that the size property accurately reflects the number of items.\"\"\"\n        cache = LRUCache(3)\n        self.assertEqual(cache.size, 0)\n        \n        cache.put(1, \"a\")\n        self.assertEqual(cache.size, 1)\n        \n        cache.put(2, \"b\")\n        cache.put(3, \"c\")\n        self.assertEqual(cache.size, 3)\n        \n        # Updating an existing key should not change size\n        cache.put(2, \"updated\")\n        self.assertEqual(cache.size, 3)\n        \n        # Inserting a new key should evict one, keeping size at capacity\n        cache.put(4, \"d\")\n        self.assertEqual(cache.size, 3)\n    \n    # ── Test 8: Contains check ────────────────────────────────────────────\n    \n    def test_contains(self) -> None:\n        \"\"\"Test the __contains__ (in) operator.\"\"\"\n        cache = LRUCache(2)\n        cache.put(\"key\", \"val\")\n        \n        self.assertIn(\"key\", cache)\n        self.assertNotIn(\"missing\", cache)\n    \n    # ── Test 9: Large sequence of operations ──────────────────────────────\n    \n    def test_large_sequence(self) -> None:\n        \"\"\"Stress test with many operations to verify correctness at scale.\"\"\"\n        cache = LRUCache(100)\n        \n        # Insert 200 items; first 100 should be evicted\n        for i in range(200):\n            cache.put(i, i * 10)\n        \n        # Keys 0-99 should have been evicted\n        for i in range(100):\n            self.assertIsNone(cache.get(i))\n        \n        # Keys 100-199 should still be present\n        for i in range(100, 200):\n            self.assertEqual(cache.get(i), i * 10)\n        \n        self.assertEqual(cache.size, 100)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\n## How It Works\n\n```\n  Doubly Linked List (access order):\n  \n  [HEAD] <──> [Most Recent] <──> [...] <──> [Least Recent] <──> [TAIL]\n  (dummy)                                                        (dummy)\n  \n  Hash Map:\n  ┌─────────┬───────────────────┐\n  │  Key    │  Pointer to Node  │\n  ├─────────┼───────────────────┤\n  │  \"a\"    │  ──────> Node(\"a\")│\n  │  \"b\"    │  ──────> Node(\"b\")│\n  │  \"c\"    │  ──────> Node(\"c\")│\n  └─────────┴───────────────────┘\n```\n\n**`get(key)`**: Look up the node in the hash map → move it to the front of the linked list → return its value.\n\n**`put(key, value)`**: If the key exists, update and move to front. If new and at capacity, remove the node just before the tail sentinel (the LRU item), delete it from the hash map, then insert the new node at the front.\n\n**Sentinel nodes** (dummy head and tail) eliminate all null-pointer edge cases when inserting/removing at boundaries.\n\n## Complexity Analysis\n\n| Aspect | Complexity | Explanation |\n|---|---|---|\n| **`get()` time** | **O(1)** |",
      "latency_ms": 50019.66225000069,
      "usage": {
        "prompt_tokens": 180,
        "completion_tokens": 4096,
        "total_tokens": 4276
      },
      "timestamp": 1770394585.5553799,
      "error": null
    },
    "gemini_3_pro": {
      "task_id": "code_generation_001",
      "model_key": "gemini_3_pro",
      "model_id": "google/gemini-3-pro-preview",
      "response": "Here is a complete implementation of an LRU Cache in Python, designed from scratch using a Hash Map and a Doubly Linked List to ensure $O(1)$ time complexity.\n\n### Implementation\n\n```python\nfrom typing import Dict, Any, Optional\nimport unittest\n\nclass DNode:\n    \"\"\"\n    Doubly Linked List Node helper class.\n    \"\"\"\n    def __init__(self, key: Any, value: Any):\n        self.key = key\n        self.value = value\n        self.prev: Optional['DNode'] = None\n        self.next: Optional['DNode'] = None\n\nclass LRUCache:\n    \"\"\"\n    Least Recently Used (LRU) Cache implementation using a Hash Map \n    and a Doubly Linked List.\n    \"\"\"\n\n    def __init__(self, capacity: int):\n        \"\"\"\n        Initialize the LRU Cache with a fixed capacity.\n        \n        Args:\n            capacity (int): The maximum number of items the cache can hold.\n        \"\"\"\n        if capacity <= 0:\n            raise ValueError(\"Capacity must be a positive integer.\")\n        \n        self.capacity = capacity\n        # Maps key -> DNode\n        self.cache: Dict[Any, DNode] = {}\n        \n        # Dummy head and tail nodes to simplify edge cases (no checking for None)\n        # Head is closest to Most Recently Used (MRU)\n        # Tail is closest to Least Recently Used (LRU)\n        self.head = DNode(0, 0)\n        self.tail = DNode(0, 0)\n        self.head.next = self.tail\n        self.tail.prev = self.head\n\n    def _add_node(self, node: DNode) -> None:\n        \"\"\"\n        Always add the new node right after head.\n        \"\"\"\n        node.prev = self.head\n        node.next = self.head.next\n        \n        self.head.next.prev = node\n        self.head.next = node\n\n    def _remove_node(self, node: DNode) -> None:\n        \"\"\"\n        Remove an existing node from the linked list.\n        \"\"\"\n        prev = node.prev\n        new = node.next\n        \n        prev.next = new\n        new.prev = prev\n\n    def _move_to_head(self, node: DNode) -> None:\n        \"\"\"\n        Move a node to the head (mark as most recently used).\n        \"\"\"\n        self._remove_node(node)\n        self._add_node(node)\n\n    def _pop_tail(self) -> DNode:\n        \"\"\"\n        Pop the current tail item (least recently used).\n        \"\"\"\n        res = self.tail.prev\n        self._remove_node(res)\n        return res\n\n    def get(self, key: Any) -> Any:\n        \"\"\"\n        Retrieve a value by key.\n        \n        Args:\n            key: The key to look up.\n            \n        Returns:\n            The value associated with the key, or -1 if the key does not exist.\n        \"\"\"\n        node = self.cache.get(key)\n        \n        if not node:\n            return -1\n        \n        # Move the accessed item to the head\n        self._move_to_head(node)\n        \n        return node.value\n\n    def put(self, key: Any, value: Any) -> None:\n        \"\"\"\n        Insert or update a key-value pair. If the cache is full, evict the\n        least recently used item.\n        \n        Args:\n            key: The key to insert.\n            value: The value to associate with the key.\n        \"\"\"\n        node = self.cache.get(key)\n\n        if node:\n            # Update the value and move to head\n            node.value = value\n            self._move_to_head(node)\n        else:\n            # Create a new node\n            new_node = DNode(key, value)\n            self.cache[key] = new_node\n            self._add_node(new_node)\n            \n            if len(self.cache) > self.capacity:\n                # Remove the LRU from the linked list\n                lru_node = self._pop_tail()\n                # Remove the LRU from the hash map\n                del self.cache[lru_node.key]\n```\n\n### Unit Tests\n\nHere are three unit tests demonstrating the correctness of the implementation, covering basic usage, eviction logic, and updates.\n\n```python\nclass TestLRUCache(unittest.TestCase):\n    \n    def test_basic_put_and_get(self):\n        \"\"\"Test basic insertion and retrieval.\"\"\"\n        lru = LRUCache(2)\n        lru.put(1, 1)\n        lru.put(2, 2)\n        \n        self.assertEqual(lru.get(1), 1)\n        self.assertEqual(lru.get(2), 2)\n        self.assertEqual(lru.get(3), -1) # Key not found\n\n    def test_eviction_policy(self):\n        \"\"\"Test that the least recently used item is evicted.\"\"\"\n        lru = LRUCache(2)\n        lru.put(1, 1)\n        lru.put(2, 2)\n        \n        # Access 1. State: [1 (MRU), 2 (LRU)]\n        lru.get(1) \n        \n        # Add 3. Capacity is 2. Evict 2. State: [3 (MRU), 1 (LRU)]\n        lru.put(3, 3) \n        \n        self.assertEqual(lru.get(2), -1) # 2 should be evicted\n        self.assertEqual(lru.get(1), 1)  # 1 should still exist\n        self.assertEqual(lru.get(3), 3)\n\n    def test_update_existing_key(self):\n        \"\"\"Test that updating a key moves it to MRU and updates value.\"\"\"\n        lru = LRUCache(2)\n        lru.put(1, 1)\n        lru.put(2, 2)\n        \n        # Update 1. State: [1 (MRU), 2 (LRU)]\n        lru.put(1, 10) \n        \n        # Add 3. Capacity is 2. Evict 2 (since 1 was just updated).\n        lru.put(3, 3)\n        \n        self.assertEqual(lru.get(1), 10) # Value updated\n        self.assertEqual(lru.get(2), -1) # 2 evicted\n        self.assertEqual(lru.get(3), 3)\n\nif __name__ == '__main__':\n    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n```\n\n### Complexity Analysis\n\n#### Time Complexity\n*   **`get(key)`**: $O(1)$\n    *   Looking up the node in the `self.cache` dictionary takes $O(1)$ on average.\n    *   Moving the node to the head of the Doubly Linked List involves changing a constant number of pointer references (prev/next), which is $O(1)$.\n*   **`put(key, value)`**: $O(1)$\n    *   Checking if the key exists in the dictionary is $O(1)$.\n    *   Adding a new node or moving an existing node to the head is $O(1)$.\n    *   If capacity is exceeded, removing the tail node is $O(1)$ because we have a direct reference to the tail via the dummy node. Deleting from the dictionary is also $O(1)$.\n\n#### Space Complexity\n*   **$O(C)$**, where $C$ is the capacity of the cache.\n    *   We store at most $C$ items in the dictionary.\n    *   We store at most $C$ nodes in the Doubly Linked List.\n    *   The overhead for the dummy head and tail nodes is constant $O(1)$.\n\n### Design Explanation\nTo achieve $O(1)$ for both operations, we combine two data structures:\n\n1.  **Hash Map (Dictionary)**: Allows us to access any node by its key instantly. However, a hash map is unordered (conceptually), so it cannot track which item was used last.\n2.  **Doubly Linked List (DLL)**: Maintains the order of items.\n    *   **Head**: Represents the Most Recently Used (MRU) end.\n    *   **Tail**: Represents the Least Recently Used (LRU) end.\n    *   We need a *Doubly* linked list (pointers to both next and prev) so that when we access an arbitrary node via the Hash Map, we can remove it from its current position and move it to the head in $O(1)$ time without traversing the list to find its predecessor.\n\n**Why Dummy Nodes?**\nUsing `self.head` and `self.tail` as dummy nodes (sentinels) avoids complex conditional logic (e.g., `if self.head is None`) when adding or removing nodes. The actual data always resides between these two sentinels.",
      "latency_ms": 36547.39245900055,
      "usage": {
        "prompt_tokens": 159,
        "completion_tokens": 3693,
        "total_tokens": 3852
      },
      "timestamp": 1770394572.096919,
      "error": null
    }
  }
}