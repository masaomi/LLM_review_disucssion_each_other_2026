{
  "task_id": "code_generation_001",
  "models": {
    "opus_4_5": {
      "task_id": "code_generation_001",
      "model_key": "opus_4_5",
      "model_id": "anthropic/claude-opus-4.5",
      "response": "# LRU Cache Implementation in Python\n\n## Overview\n\nI'll implement an LRU Cache using a **doubly linked list** combined with a **hash map**. This combination allows O(1) access, insertion, and deletion operations.\n\n## Implementation\n\n```python\nfrom typing import Optional, TypeVar, Generic, Dict\n\nK = TypeVar('K')\nV = TypeVar('V')\n\n\nclass Node(Generic[K, V]):\n    \"\"\"\n    A node in the doubly linked list.\n    \n    Stores key-value pairs and maintains pointers to previous and next nodes.\n    The key is stored to enable O(1) removal from the hash map during eviction.\n    \"\"\"\n    \n    def __init__(self, key: K, value: V) -> None:\n        self.key: K = key\n        self.value: V = value\n        self.prev: Optional['Node[K, V]'] = None\n        self.next: Optional['Node[K, V]'] = None\n\n\nclass LRUCache(Generic[K, V]):\n    \"\"\"\n    A Least Recently Used (LRU) Cache implementation.\n    \n    Uses a doubly linked list to maintain access order and a hash map for O(1) lookups.\n    The most recently used item is at the head, least recently used at the tail.\n    \n    Attributes:\n        capacity: Maximum number of items the cache can hold.\n        \n    Example:\n        >>> cache = LRUCache[int, str](capacity=2)\n        >>> cache.put(1, \"one\")\n        >>> cache.put(2, \"two\")\n        >>> cache.get(1)\n        'one'\n        >>> cache.put(3, \"three\")  # Evicts key 2\n        >>> cache.get(2)  # Returns None, was evicted\n    \"\"\"\n    \n    def __init__(self, capacity: int) -> None:\n        \"\"\"\n        Initialize the LRU Cache.\n        \n        Args:\n            capacity: Maximum number of items the cache can store.\n                     Must be a positive integer.\n                     \n        Raises:\n            ValueError: If capacity is less than 1.\n        \"\"\"\n        if capacity < 1:\n            raise ValueError(\"Capacity must be at least 1\")\n        \n        self.capacity: int = capacity\n        self._cache: Dict[K, Node[K, V]] = {}\n        \n        # Dummy head and tail nodes to simplify edge cases\n        # head <-> [actual nodes] <-> tail\n        self._head: Node = Node(None, None)  # type: ignore\n        self._tail: Node = Node(None, None)  # type: ignore\n        self._head.next = self._tail\n        self._tail.prev = self._head\n    \n    def _remove_node(self, node: Node[K, V]) -> None:\n        \"\"\"\n        Remove a node from the doubly linked list.\n        \n        Args:\n            node: The node to remove from the list.\n        \"\"\"\n        prev_node = node.prev\n        next_node = node.next\n        \n        if prev_node:\n            prev_node.next = next_node\n        if next_node:\n            next_node.prev = prev_node\n            \n        node.prev = None\n        node.next = None\n    \n    def _add_to_front(self, node: Node[K, V]) -> None:\n        \"\"\"\n        Add a node right after the head (most recently used position).\n        \n        Args:\n            node: The node to add to the front of the list.\n        \"\"\"\n        node.prev = self._head\n        node.next = self._head.next\n        \n        if self._head.next:\n            self._head.next.prev = node\n        self._head.next = node\n    \n    def _move_to_front(self, node: Node[K, V]) -> None:\n        \"\"\"\n        Move an existing node to the front (mark as most recently used).\n        \n        Args:\n            node: The node to move to the front.\n        \"\"\"\n        self._remove_node(node)\n        self._add_to_front(node)\n    \n    def _evict_lru(self) -> None:\n        \"\"\"\n        Remove the least recently used item (node before tail).\n        \"\"\"\n        lru_node = self._tail.prev\n        \n        # Don't evict the dummy head\n        if lru_node and lru_node != self._head:\n            self._remove_node(lru_node)\n            del self._cache[lru_node.key]\n    \n    def get(self, key: K) -> Optional[V]:\n        \"\"\"\n        Retrieve a value from the cache.\n        \n        If the key exists, it becomes the most recently used item.\n        \n        Args:\n            key: The key to look up.\n            \n        Returns:\n            The value associated with the key, or None if not found.\n            \n        Time Complexity: O(1)\n        \"\"\"\n        if key not in self._cache:\n            return None\n        \n        node = self._cache[key]\n        self._move_to_front(node)\n        return node.value\n    \n    def put(self, key: K, value: V) -> None:\n        \"\"\"\n        Insert or update a key-value pair in the cache.\n        \n        If the key already exists, updates the value and marks it as most recently used.\n        If the cache is at capacity, evicts the least recently used item before inserting.\n        \n        Args:\n            key: The key to insert or update.\n            value: The value to associate with the key.\n            \n        Time Complexity: O(1)\n        \"\"\"\n        if key in self._cache:\n            # Update existing node\n            node = self._cache[key]\n            node.value = value\n            self._move_to_front(node)\n        else:\n            # Check if we need to evict\n            if len(self._cache) >= self.capacity:\n                self._evict_lru()\n            \n            # Create and add new node\n            new_node = Node(key, value)\n            self._cache[key] = new_node\n            self._add_to_front(new_node)\n    \n    def __len__(self) -> int:\n        \"\"\"Return the current number of items in the cache.\"\"\"\n        return len(self._cache)\n    \n    def __contains__(self, key: K) -> bool:\n        \"\"\"Check if a key exists in the cache (without affecting LRU order).\"\"\"\n        return key in self._cache\n    \n    def __repr__(self) -> str:\n        \"\"\"Return a string representation showing items from MRU to LRU.\"\"\"\n        items = []\n        current = self._head.next\n        while current and current != self._tail:\n            items.append(f\"{current.key}: {current.value}\")\n            current = current.next\n        return f\"LRUCache([{', '.join(items)}], capacity={self.capacity})\"\n```\n\n## Unit Tests\n\n```python\nimport unittest\n\n\nclass TestLRUCache(unittest.TestCase):\n    \"\"\"Comprehensive unit tests for LRU Cache implementation.\"\"\"\n    \n    def test_basic_get_and_put(self):\n        \"\"\"Test basic get and put operations work correctly.\"\"\"\n        cache: LRUCache[int, str] = LRUCache(capacity=3)\n        \n        # Test put and get\n        cache.put(1, \"one\")\n        cache.put(2, \"two\")\n        cache.put(3, \"three\")\n        \n        self.assertEqual(cache.get(1), \"one\")\n        self.assertEqual(cache.get(2), \"two\")\n        self.assertEqual(cache.get(3), \"three\")\n        self.assertEqual(len(cache), 3)\n        \n        # Test get for non-existent key\n        self.assertIsNone(cache.get(4))\n    \n    def test_eviction_policy(self):\n        \"\"\"Test that LRU item is evicted when cache is full.\"\"\"\n        cache: LRUCache[int, str] = LRUCache(capacity=2)\n        \n        cache.put(1, \"one\")\n        cache.put(2, \"two\")\n        \n        # Cache is now full: [2, 1] (2 is MRU, 1 is LRU)\n        self.assertEqual(len(cache), 2)\n        \n        # Adding new item should evict key 1 (LRU)\n        cache.put(3, \"three\")\n        \n        self.assertEqual(len(cache), 2)\n        self.assertIsNone(cache.get(1))  # 1 was evicted\n        self.assertEqual(cache.get(2), \"two\")\n        self.assertEqual(cache.get(3), \"three\")\n    \n    def test_access_updates_recency(self):\n        \"\"\"Test that accessing an item makes it most recently used.\"\"\"\n        cache: LRUCache[int, str] = LRUCache(capacity=2)\n        \n        cache.put(1, \"one\")\n        cache.put(2, \"two\")\n        \n        # Access key 1, making it MRU (order now: [1, 2])\n        self.assertEqual(cache.get(1), \"one\")\n        \n        # Add key 3, should evict key 2 (now LRU)\n        cache.put(3, \"three\")\n        \n        self.assertEqual(cache.get(1), \"one\")  # Still exists\n        self.assertIsNone(cache.get(2))  # Was evicted\n        self.assertEqual(cache.get(3), \"three\")\n    \n    def test_update_existing_key(self):\n        \"\"\"Test that updating existing key updates value and recency.\"\"\"\n        cache: LRUCache[str, int] = LRUCache(capacity=2)\n        \n        cache.put(\"a\", 1)\n        cache.put(\"b\", 2)\n        \n        # Update \"a\" - should become MRU and have new value\n        cache.put(\"a\", 100)\n        \n        self.assertEqual(cache.get(\"a\"), 100)\n        self.assertEqual(len(cache), 2)\n        \n        # Add \"c\", should evict \"b\" (LRU after \"a\" was updated)\n        cache.put(\"c\", 3)\n        \n        self.assertIsNone(cache.get(\"b\"))\n        self.assertEqual(cache.get(\"a\"), 100)\n        self.assertEqual(cache.get(\"c\"), 3)\n    \n    def test_capacity_one(self):\n        \"\"\"Test cache with capacity of 1.\"\"\"\n        cache: LRUCache[int, int] = LRUCache(capacity=1)\n        \n        cache.put(1, 100)\n        self.assertEqual(cache.get(1), 100)\n        \n        cache.put(2, 200)\n        self.assertIsNone(cache.get(1))  # Evicted\n        self.assertEqual(cache.get(2), 200)\n    \n    def test_invalid_capacity(self):\n        \"\"\"Test that invalid capacity raises ValueError.\"\"\"\n        with self.assertRaises(ValueError):\n            LRUCache(capacity=0)\n        \n        with self.assertRaises(ValueError):\n            LRUCache(capacity=-1)\n    \n    def test_complex_access_pattern(self):\n        \"\"\"Test complex sequence of operations.\"\"\"\n        cache: LRUCache[int, int] = LRUCache(capacity=3)\n        \n        # Fill cache\n        cache.put(1, 1)\n        cache.put(2, 2)\n        cache.put(3, 3)  # Order: [3, 2, 1]\n        \n        # Access 1, making it MRU: [1, 3, 2]\n        self.assertEqual(cache.get(1), 1)\n        \n        # Access 2, making it MRU: [2, 1, 3]\n        self.assertEqual(cache.get(2), 2)\n        \n        # Add 4, evicting 3 (LRU): [4, 2, 1]\n        cache.put(4, 4)\n        \n        self.assertIsNone(cache.get(3))  # Evicted\n        self.assertEqual(cache.get(1), 1)\n        self.assertEqual(cache.get(2), 2)\n        self.assertEqual(cache.get(4), 4)\n    \n    def test_contains_and_len(self):\n        \"\"\"Test __contains__ and __len__ methods.\"\"\"\n        cache: LRUCache[str, str] = LRUCache(capacity=2)\n        \n        self.assertEqual(len(cache), 0)\n        self.assertFalse(\"key\" in cache)\n        \n        cache.put(\"key\", \"value\")\n        \n        self.assertEqual(len(cache), 1)\n        self.assertTrue(\"key\" in cache)\n        self.assertFalse(\"other\" in cache)\n\n\nif __name__ == \"__main__\":\n    unittest.main(verbosity=2)\n```\n\n## Running the Tests\n\n```python\n# Run this to execute all tests\nif __name__ == \"__main__\":\n    unittest.main(verbosity=2)\n```\n\n**Expected Output:**\n```\ntest_access_updates_recency (__main__.TestLRUCache) ... ok\ntest_basic_get_and_put (__main__.TestLRUCache) ... ok\ntest_capacity_one (__main__.TestLRUCache) ... ok\ntest_complex_access_pattern (__main__.TestLRUCache) ... ok\ntest_contains_and_len (__main__.TestLRUCache) ... ok\ntest_eviction_policy (__main__.TestLRUCache) ... ok\ntest_invalid_capacity (__main__.TestLRUCache) ... ok\ntest_update_existing_key (__main__.TestLRUCache) ... ok\n\n----------------------------------------------------------------------\nRan 8 tests in 0.001s\n\nOK\n```\n\n## Time and Space Complexity Analysis\n\n### Time Complexity\n\n| Operation | Complexity | Explanation |\n|-----------|------------|-------------|\n| `get(key)` | **O(1)** | Hash map lookup is O(1). Moving node to front involves only pointer updates (O(1)). |\n| `put(key, value)` | **O(1)** | Hash map insertion/lookup is O(1). Adding/removing nodes from doubly linked list is O(1) with direct node references. |\n\n**Why O(1)?**\n- **Hash Map**: Provides O(1) average-case lookup, insertion, and deletion by key.\n- **Doubly Linked List**: With direct node references (stored in the hash map), we can:\n  - Remove any node in O(1) by updating its neighbors' pointers\n  - Add to front in O(1) by updating head's pointers\n  - No traversal is ever needed\n\n### Space Complexity\n\n| Component | Space | Explanation |\n|-----------|-------|-------------|\n| Hash Map | O(n) | Stores up to `capacity` key-node pairs |\n| Linked List | O(n) | Each of the `n` items has a node with key, value, and 2 pointers |\n| Dummy Nodes | O(1) | Fixed 2 sentinel nodes regardless of cache size |\n| **Total** | **O(n)** | Where n = capacity of the cache |\n\n### Data Structure Visualization\n\n```\nHash Map:  {key1: Node1, key2: Node2, key3: Node3}\n                ↓          ↓          ↓\nLinked List:\n    HEAD ←→ [Node1] ←→ [Node2] ←→ [Node3] ←→ TAIL\n            (MRU)                   (LRU)\n```\n\nThe combination of these two data structures gives us the best of both worlds:\n- **Hash Map**: Fast key-based access\n- **Doubly Linked List**: Efficient ordering and removal from any position",
      "latency_ms": 45540.979833000165,
      "usage": {
        "prompt_tokens": 179,
        "completion_tokens": 3877,
        "total_tokens": 4056
      },
      "timestamp": 1770389190.476483,
      "error": null
    },
    "opus_4_6": {
      "task_id": "code_generation_001",
      "model_key": "opus_4_6",
      "model_id": "anthropic/claude-opus-4.6",
      "response": "\n\n# LRU Cache Implementation\n\n## Implementation\n\n```python\n\"\"\"\nLRU (Least Recently Used) Cache implementation using a doubly linked list\nand a hash map for O(1) get and put operations.\n\"\"\"\n\nfrom typing import Optional, Any, Dict\n\n\nclass _DLinkedNode:\n    \"\"\"A node in the doubly linked list used internally by the LRU Cache.\n    \n    Attributes:\n        key: The key stored in this node (needed for eviction lookup).\n        value: The value stored in this node.\n        prev: Pointer to the previous node in the list.\n        next: Pointer to the next node in the list.\n    \"\"\"\n    __slots__ = ('key', 'value', 'prev', 'next')\n\n    def __init__(self, key: int = 0, value: int = 0) -> None:\n        self.key: int = key\n        self.value: int = value\n        self.prev: Optional['_DLinkedNode'] = None\n        self.next: Optional['_DLinkedNode'] = None\n\n\nclass LRUCache:\n    \"\"\"A Least Recently Used (LRU) Cache with O(1) get and put operations.\n    \n    The cache combines a hash map (dict) with a doubly linked list:\n    - The hash map provides O(1) key lookup to find nodes.\n    - The doubly linked list maintains access order, with the most recently\n      used item at the head and the least recently used item at the tail.\n    - Sentinel head and tail nodes simplify edge-case handling for\n      insertions and removals.\n    \n    When the cache exceeds its capacity on a `put`, the node just before\n    the tail sentinel (the least recently used item) is evicted.\n    \n    Args:\n        capacity: The maximum number of key-value pairs the cache can hold.\n                  Must be a positive integer.\n    \n    Raises:\n        ValueError: If capacity is not a positive integer.\n    \n    Example:\n        >>> cache = LRUCache(2)\n        >>> cache.put(1, 10)\n        >>> cache.put(2, 20)\n        >>> cache.get(1)\n        10\n        >>> cache.put(3, 30)  # evicts key 2\n        >>> cache.get(2)      # returns -1 (not found)\n        -1\n    \"\"\"\n\n    def __init__(self, capacity: int) -> None:\n        if capacity <= 0:\n            raise ValueError(f\"Capacity must be a positive integer, got {capacity}\")\n\n        self._capacity: int = capacity\n        self._size: int = 0\n        self._cache: Dict[int, _DLinkedNode] = {}\n\n        # Sentinel nodes — they never hold real data.\n        # head.next is the MRU node; tail.prev is the LRU node.\n        self._head: _DLinkedNode = _DLinkedNode()\n        self._tail: _DLinkedNode = _DLinkedNode()\n        self._head.next = self._tail\n        self._tail.prev = self._head\n\n    # ------------------------------------------------------------------ #\n    #  Private helpers for doubly-linked-list manipulation                #\n    # ------------------------------------------------------------------ #\n\n    def _add_node(self, node: _DLinkedNode) -> None:\n        \"\"\"Insert a node right after the head sentinel (MRU position).\n        \n        Args:\n            node: The node to insert.\n        \"\"\"\n        node.prev = self._head\n        node.next = self._head.next\n\n        self._head.next.prev = node\n        self._head.next = node\n\n    def _remove_node(self, node: _DLinkedNode) -> None:\n        \"\"\"Remove an existing node from the linked list.\n        \n        Args:\n            node: The node to remove. Must currently be in the list.\n        \"\"\"\n        prev_node = node.prev\n        next_node = node.next\n\n        prev_node.next = next_node\n        next_node.prev = prev_node\n\n    def _move_to_head(self, node: _DLinkedNode) -> None:\n        \"\"\"Move an existing node to the head (MRU position).\n        \n        Args:\n            node: The node to move.\n        \"\"\"\n        self._remove_node(node)\n        self._add_node(node)\n\n    def _pop_tail(self) -> _DLinkedNode:\n        \"\"\"Remove and return the node just before the tail sentinel (LRU).\n        \n        Returns:\n            The least recently used node that was removed.\n        \"\"\"\n        lru_node = self._tail.prev\n        self._remove_node(lru_node)\n        return lru_node\n\n    # ------------------------------------------------------------------ #\n    #  Public API                                                         #\n    # ------------------------------------------------------------------ #\n\n    def get(self, key: int) -> int:\n        \"\"\"Retrieve the value associated with the given key.\n        \n        Accessing a key marks it as the most recently used item.\n        \n        Args:\n            key: The key to look up.\n        \n        Returns:\n            The value associated with the key, or -1 if the key is not\n            present in the cache.\n        \"\"\"\n        node = self._cache.get(key)\n        if node is None:\n            return -1\n\n        # Mark as most recently used\n        self._move_to_head(node)\n        return node.value\n\n    def put(self, key: int, value: int) -> None:\n        \"\"\"Insert or update a key-value pair in the cache.\n        \n        If the key already exists, its value is updated and it becomes the\n        most recently used item. If the key is new and the cache is at\n        capacity, the least recently used item is evicted first.\n        \n        Args:\n            key: The key to insert or update.\n            value: The value to associate with the key.\n        \"\"\"\n        node = self._cache.get(key)\n\n        if node is not None:\n            # Update existing entry\n            node.value = value\n            self._move_to_head(node)\n        else:\n            # Create new entry\n            new_node = _DLinkedNode(key, value)\n            self._cache[key] = new_node\n            self._add_node(new_node)\n            self._size += 1\n\n            if self._size > self._capacity:\n                # Evict the least recently used item\n                tail_node = self._pop_tail()\n                del self._cache[tail_node.key]\n                self._size -= 1\n\n    def __len__(self) -> int:\n        \"\"\"Return the current number of items in the cache.\"\"\"\n        return self._size\n\n    def __contains__(self, key: int) -> bool:\n        \"\"\"Check if a key is in the cache (without affecting access order).\n        \n        Args:\n            key: The key to check.\n        \n        Returns:\n            True if the key is present, False otherwise.\n        \"\"\"\n        return key in self._cache\n\n    def __repr__(self) -> str:\n        \"\"\"Return a string representation showing items from MRU to LRU.\"\"\"\n        items = []\n        current = self._head.next\n        while current is not self._tail:\n            items.append(f\"{current.key}: {current.value}\")\n            current = current.next\n        return f\"LRUCache(capacity={self._capacity}, items=[{', '.join(items)}])\"\n\n\n# ====================================================================== #\n#  Unit Tests                                                             #\n# ====================================================================== #\n\nimport unittest\n\n\nclass TestLRUCache(unittest.TestCase):\n    \"\"\"Comprehensive tests for the LRU Cache implementation.\"\"\"\n\n    # ---- Test 1: Basic get/put and eviction ---- #\n    def test_basic_operations_and_eviction(self) -> None:\n        \"\"\"Test that basic put/get work and LRU item is evicted when full.\"\"\"\n        cache = LRUCache(2)\n\n        cache.put(1, 1)\n        cache.put(2, 2)\n        self.assertEqual(cache.get(1), 1)   # key 1 exists → returns 1\n\n        # Cache is full (capacity 2). Inserting key 3 should evict key 2\n        # because key 1 was accessed more recently than key 2.\n        cache.put(3, 3)\n        self.assertEqual(cache.get(2), -1)  # key 2 was evicted\n        self.assertEqual(cache.get(3), 3)   # key 3 exists\n\n    # ---- Test 2: Update existing key ---- #\n    def test_update_existing_key(self) -> None:\n        \"\"\"Test that putting an existing key updates its value and\n        refreshes its position (does not cause spurious eviction).\"\"\"\n        cache = LRUCache(2)\n\n        cache.put(1, 1)\n        cache.put(2, 2)\n\n        # Update key 1 — this should NOT increase size\n        cache.put(1, 10)\n        self.assertEqual(len(cache), 2)\n        self.assertEqual(cache.get(1), 10)\n\n        # Now insert key 3; key 2 should be evicted (LRU), not key 1\n        cache.put(3, 3)\n        self.assertEqual(cache.get(2), -1)  # evicted\n        self.assertEqual(cache.get(1), 10)  # still present\n        self.assertEqual(cache.get(3), 3)   # still present\n\n    # ---- Test 3: Access order affects eviction ---- #\n    def test_access_order_determines_eviction(self) -> None:\n        \"\"\"Test that `get` refreshes access order so the correct item\n        is evicted.\"\"\"\n        cache = LRUCache(3)\n\n        cache.put(1, 1)\n        cache.put(2, 2)\n        cache.put(3, 3)\n\n        # Access key 1 — now order (MRU→LRU): 1, 3, 2\n        cache.get(1)\n\n        # Insert key 4 — should evict key 2 (the LRU)\n        cache.put(4, 4)\n        self.assertEqual(cache.get(2), -1)  # evicted\n        self.assertEqual(cache.get(1), 1)\n        self.assertEqual(cache.get(3), 3)\n        self.assertEqual(cache.get(4), 4)\n\n    # ---- Test 4: Capacity of 1 ---- #\n    def test_capacity_one(self) -> None:\n        \"\"\"Edge case: cache with capacity 1 should always hold only\n        the most recent item.\"\"\"\n        cache = LRUCache(1)\n\n        cache.put(1, 1)\n        self.assertEqual(cache.get(1), 1)\n\n        cache.put(2, 2)\n        self.assertEqual(cache.get(1), -1)  # evicted\n        self.assertEqual(cache.get(2), 2)\n\n    # ---- Test 5: Get on missing key ---- #\n    def test_get_missing_key(self) -> None:\n        \"\"\"Getting a key that was never inserted returns -1.\"\"\"\n        cache = LRUCache(5)\n        self.assertEqual(cache.get(42), -1)\n\n    # ---- Test 6: Stress / sequential eviction ---- #\n    def test_sequential_eviction(self) -> None:\n        \"\"\"Insert more items than capacity and verify only the most\n        recent `capacity` items remain.\"\"\"\n        cap = 3\n        cache = LRUCache(cap)\n\n        for i in range(10):\n            cache.put(i, i * 100)\n\n        # Only keys 7, 8, 9 should remain\n        for i in range(7):\n            self.assertEqual(cache.get(i), -1)\n        for i in range(7, 10):\n            self.assertEqual(cache.get(i), i * 100)\n\n        self.assertEqual(len(cache), cap)\n\n    # ---- Test 7: Invalid capacity ---- #\n    def test_invalid_capacity_raises(self) -> None:\n        \"\"\"Creating a cache with non-positive capacity should raise.\"\"\"\n        with self.assertRaises(ValueError):\n            LRUCache(0)\n        with self.assertRaises(ValueError):\n            LRUCache(-5)\n\n    # ---- Test 8: __contains__ does not affect order ---- #\n    def test_contains_does_not_affect_order(self) -> None:\n        \"\"\"The `in` operator should not change access order.\"\"\"\n        cache = LRUCache(2)\n        cache.put(1, 1)\n        cache.put(2, 2)\n\n        # Check containment of key 1 (should NOT refresh it)\n        self.assertIn(1, cache)\n\n        # Insert key 3 — key 1 should be evicted because `in` didn't\n        # refresh its access time; key 1 is still LRU\n        cache.put(3, 3)\n        self.assertEqual(cache.get(1), -1)\n        self.assertEqual(cache.get(2), 2)\n        self.assertEqual(cache.get(3), 3)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\n---\n\n## How It Works\n\n```\n  Doubly Linked List (access order):\n\n  [HEAD sentinel] <--> [MRU node] <--> ... <--> [LRU node] <--> [TAIL sentinel]\n\n  Hash Map:\n  { key → pointer to DLinkedNode in the list }\n```\n\n**`get(key)`**: Look up the node in the hash map. If found, move it to the head (MRU position) and return its value. Otherwise return -1.\n\n**`put(key, value)`**: If the key exists, update the node's value and move it to the head. If the key is new, create a node, add it at the head, and insert it into the hash map. If this causes the size to exceed capacity, pop the tail node (LRU) and remove it from the hash map.\n\n---\n\n## Complexity Analysis\n\n| Aspect | Complexity | Explanation |\n|--------|-----------|-------------|\n| **`get` time** | **O(1)** | Hash map lookup is O(1). Moving a node to the head involves pointer reassignment — O(1). |\n| **`put` time** | **O(1)** | Hash map insert/lookup is O(1). Adding a node at the head or removing the tail node are all O(1) pointer operations. |\n| **Space** | **O(capacity)** | The hash map stores at most `capacity` entries. The doubly linked list has at most `capacity` real nodes plus 2 sentinel nodes. Each node uses O(1) space. Total: **O(capacity)**. |\n\n### Why this design?\n\n- **Hash map alone** gives O(1) lookup but no ordering information.\n- **Linked list alone** gives O(1) insertion/removal at ends but O(n) search.\n- **Combined**, the hash map provides instant access to any node, and the doubly linked list maintains recency order with O(1) move/remove/insert operations. The sentinel nodes eliminate all null-check edge cases when the list is empty or has one element.",
      "latency_ms": 48485.16608299906,
      "usage": {
        "prompt_tokens": 180,
        "completion_tokens": 3694,
        "total_tokens": 3874
      },
      "timestamp": 1770389193.454628,
      "error": null
    }
  }
}